{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ba48383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "\n",
    "question = \"\"\"\n",
    "In adult patients presenting to the emergency department with suspected sepsis, what is the reported \n",
    "impact of using point-of-care procalcitonin (PCT) testing, compared to standard central laboratory PCT testing, \n",
    "on time to antibiotic administration and key clinical outcomes such as in-hospital mortality and length of stay?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec675333",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tanmayshubhgarg/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/tanmayshubhgarg/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tanmayshubhgarg/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/tanmayshubhgarg/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "sw_nltk = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def keyword_generator_stopwords(question):\n",
    "    question = question.lower()\n",
    "    words = nltk.word_tokenize(question)\n",
    "    words_no_punct = [re.sub(r'[^\\w\\s]', '', word) for word in words]\n",
    "    words_no_punct = [word for word in words_no_punct if word]\n",
    "    filtered_words = [word for word in words_no_punct if word not in sw_nltk]\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "    keyword = ' '.join(lemmatized_words)\n",
    "    return keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a4bba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "without_stopwords = keyword_generator_stopwords(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0449072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are an expert AI assistant that generates a search query for the arXiv database. Your goal is to convert a user's question into a syntactically perfect and logically optimal search query string.\n",
    "\n",
    "First, analyze the user's question to identify the core concepts, keywords, and important relationships.\n",
    "\n",
    "Then, construct a single search query string based on those concepts. The query string **must** adhere to the following strict formatting rules:\n",
    "\n",
    "1.  **Uppercase Operators**: All boolean operators **must** be in uppercase: `AND`, `OR`, `ANDNOT`.\n",
    "2.  **Quote All Phrases**: Any search term containing more than one word **must** be enclosed in double quotes (`\"`). For example, search for `\"point-of-care testing\"`, not `point-of-care testing`.\n",
    "3.  **Group with Parentheses**: Use parentheses `()` to logically group related concepts, especially when mixing `AND` and `OR`.\n",
    "4.  **Combine Synonyms & Acronyms**: Use the `OR` operator inside parentheses to search for multiple variations of a single concept (e.g., synonyms, acronyms, or different spellings). For example: `(\"procalcitonin\" OR \"pct\")`.\n",
    "5.  **Connect Core Concepts with AND**: Link the main, distinct ideas of the search together using the `AND` operator to ensure all concepts are present in the results.\n",
    "\n",
    "The user's question is: \"{question}\"\n",
    "The question with stopwords removed is: \"{without_stopwords}\"\n",
    "\n",
    "Now, generate the single, optimal search query string. Your output must be **only the search term string itself**, with no additional explanation, preamble, or text.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ce0c15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an expert AI assistant that generates a search query for the arXiv database. Your goal is to convert a user's question into a syntactically perfect and logically optimal search query string.\n",
      "\n",
      "First, analyze the user's question to identify the core concepts, keywords, and important relationships.\n",
      "\n",
      "Then, construct a single search query string based on those concepts. The query string **must** adhere to the following strict formatting rules:\n",
      "\n",
      "1.  **Uppercase Operators**: All boolean operators **must** be in uppercase: `AND`, `OR`, `ANDNOT`.\n",
      "2.  **Quote All Phrases**: Any search term containing more than one word **must** be enclosed in double quotes (`\"`). For example, search for `\"point-of-care testing\"`, not `point-of-care testing`.\n",
      "3.  **Group with Parentheses**: Use parentheses `()` to logically group related concepts, especially when mixing `AND` and `OR`.\n",
      "4.  **Combine Synonyms & Acronyms**: Use the `OR` operator inside parentheses to search for multiple variations of a single concept (e.g., synonyms, acronyms, or different spellings). For example: `(\"procalcitonin\" OR \"pct\")`.\n",
      "5.  **Connect Core Concepts with AND**: Link the main, distinct ideas of the search together using the `AND` operator to ensure all concepts are present in the results.\n",
      "\n",
      "The user's question is: \"\n",
      "In adult patients presenting to the emergency department with suspected sepsis, what is the reported \n",
      "impact of using point-of-care procalcitonin (PCT) testing, compared to standard central laboratory PCT testing, \n",
      "on time to antibiotic administration and key clinical outcomes such as in-hospital mortality and length of stay?\n",
      "\"\n",
      "The question with stopwords removed is: \"adult patient presenting emergency department suspected sepsis reported impact using pointofcare procalcitonin pct testing compared standard central laboratory pct testing time antibiotic administration key clinical outcome inhospital mortality length stay\"\n",
      "\n",
      "Now, generate the single, optimal search query string. Your output must be **only the search term string itself**, with no additional explanation, preamble, or text.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df860fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "endpoint = \"https://aoai-camp.openai.azure.com/\"\n",
    "model_name = \"gpt-4o-mini\"\n",
    "deployment = \"abbott_researcher\"\n",
    "subscription_key = os.getenv(\"AZURE_OPEN_AI_KEY\")\n",
    "api_version = \"2024-12-01-preview\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f3ed5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT: (\"adult patients\" OR \"adults\") AND (\"emergency department\" OR \"emergency room\") AND \"suspected sepsis\" AND (\"point-of-care\" OR \"POC\") AND (\"procalcitonin\" OR \"pct\") AND (\"central laboratory\" OR \"standard laboratory\") AND (\"time to antibiotic administration\" OR \"time until antibiotic administration\") AND (\"in-hospital mortality\" OR \"mortality\") AND (\"length of stay\" OR \"hospital stay\")\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "reply = chat_completion.choices[0].message.content\n",
    "print(f\"ChatGPT: {reply}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "309e7eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_keywords = reply\n",
    "keyword = llm_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "138508bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e88b4de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = \"diagnostic device\"\n",
    "num_articles = 100\n",
    "encodingmethod = \"utf-8\"\n",
    "errortype = \"strict\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c6b5199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for 'diagnostic device' on arXiv...\n",
      "URL: http://export.arxiv.org/api/query?search_query=all:diagnostic%20device&start=0&max_results=100\n",
      "Successfully retrieved search results!\n",
      "Successfully retrieved search results!\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "import urllib.request\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "encoded_search_term = urllib.parse.quote(keyword, encoding=encodingmethod, errors=errortype)\n",
    "url = f'http://export.arxiv.org/api/query?search_query=all:{encoded_search_term}&start=0&max_results={num_articles}'\n",
    "\n",
    "print(f\"Searching for '{keyword}' on arXiv...\")\n",
    "print(f\"URL: {url}\")\n",
    "\n",
    "try:\n",
    "    response = urllib.request.urlopen(url)\n",
    "    try:\n",
    "        url_read = response.read().decode(\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        response = urllib.request.urlopen(url)\n",
    "        url_read = response.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    parse_xml = ET.fromstring(url_read)\n",
    "    print(\"Successfully retrieved search results!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fdf88777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 articles with PDF links\n",
      "1. An Electrochemical Potentiostat Interface for Mobile Devices: Enabling\n",
      "  Remote ...\n",
      "2. Practical Statistical Considerations for the Clinical Validation of\n",
      "  AI/ML-enab...\n",
      "3. Cross-device Federated Learning for Mobile Health Diagnostics: A First\n",
      "  Study o...\n",
      "4. Random Forests for Industrial Device Functioning Diagnostics Using\n",
      "  Wireless Se...\n",
      "5. D-Mag: a laboratory for studying plasma physics and diagnostics in\n",
      "  strong magn...\n",
      "6. Noninvasive Acute Compartment Syndrome Diagnosis Using Random Forest\n",
      "  Machine L...\n",
      "7. Diagnostic criterion for crystallized beams...\n",
      "8. Plasma diagnostics using digital holographic interferometry...\n",
      "9. Bioimpedance a Diagnostic Tool for Tobacco Induced Oral Lesions: a Mixed\n",
      "  Model...\n",
      "10. Active Sampling for MRI-based Sequential Decision Making...\n",
      "11. Conceptual Study of a Collective Thomson Scattering Diagnostic for SPARC...\n",
      "12. Integrated Data Analysis and Validation...\n",
      "13. Special behavior of alkali beam emission spectroscopy in\n",
      "  low-ion-temperature p...\n",
      "14. Design considerations for an ultrahigh-bandwidth Phase 6 Contrast\n",
      "  Imaging syst...\n",
      "15. Predicting proximity with ambient mobile sensors for non-invasive health\n",
      "  diagn...\n",
      "16. On-line Remote EKG as a Web Service...\n",
      "17. Thomson backscattering diagnostics of nanosecond electron bunches in\n",
      "  high spac...\n",
      "18. A likelihood based sensitivity analysis for publication bias on summary\n",
      "  ROC in...\n",
      "19. A new technique for tokamak edge density measurement based on microwave\n",
      "  interf...\n",
      "20. Cybersecurity and Frequent Cyber Attacks on IoT Devices in Healthcare:\n",
      "  Issues ...\n",
      "21. SNS Front End Diagnostics...\n",
      "22. Field Effect Transistor Nanosensor for Breast Cancer Diagnostics...\n",
      "23. Conceptual design of electron beam diagnostics for high brightness\n",
      "  plasma acce...\n",
      "24. Deep Learning for Plasma Tomography and Disruption Prediction from\n",
      "  Bolometer D...\n",
      "25. Inside Job: Diagnosing Bluetooth Lower Layers Using Off-the-Shelf\n",
      "  Devices...\n",
      "26. Large Language Models in Ambulatory Devices for Home Health Diagnostics:\n",
      "  A cas...\n",
      "27. Advances in Biomedical Devices_A comprehensive Exploration of\n",
      "  Cardiovascular a...\n",
      "28. Machine-Learning-Powered Neural Interfaces for Smart Prosthetics and\n",
      "  Diagnosti...\n",
      "29. Role of Edge Device and Cloud Machine Learning in Point-of-Care\n",
      "  Solutions Usin...\n",
      "30. Resolving Two Beams in Beam Splitters with a Beam Position Monitor...\n",
      "31. Vibration diagnostics instrumentation for ILC...\n",
      "32. Surface plasmon based thermo-optic and temperature sensor for\n",
      "  microfluidic the...\n",
      "33. A contactless microwave-based diagnostic tool for high repetition rate\n",
      "  laser s...\n",
      "34. Faithful hierarchy of genuine $n$-photon quantum non-Gaussian light...\n",
      "35. A multiple testing framework for diagnostic accuracy studies with\n",
      "  co-primary e...\n",
      "36. An Integrated AI-Enabled System Using One Class Twin Cross Learning\n",
      "  (OCT-X) fo...\n",
      "37. Can User-Level Probing Detect and Diagnose Common Home-WLAN Pathologies?...\n",
      "38. Whole Device Modeling of the FuZE Sheared-Flow-Stabilized Z Pinch...\n",
      "39. Model Compression Engine for Wearable Devices Skin Cancer Diagnosis...\n",
      "40. Designing a Deep Learning-Driven Resource-Efficient Diagnostic System\n",
      "  for Meta...\n",
      "41. Qubit assignment using time reversal...\n",
      "42. Beyond One-Time Validation: A Framework for Adaptive Validation of\n",
      "  Prognostic ...\n",
      "43. Multiplexed Biosensing of Proteins and Virions with Disposable Plasmonic\n",
      "  Assay...\n",
      "44. Systems-Theoretic and Data-Driven Security Analysis in ML-enabled\n",
      "  Medical Devi...\n",
      "45. Image-guided therapy system for interstitial gynecologic brachytherapy\n",
      "  in a mu...\n",
      "46. Measuring national capability over big sciences multidisciplinarity: A\n",
      "  case st...\n",
      "47. Rapid, remote and low-cost finger vasculature mapping for heart rate\n",
      "  monitorin...\n",
      "48. Beyond the Nucleus: Cytoplasmic Dominance in Follicular Thyroid\n",
      "  Carcinoma Dete...\n",
      "49. DrHouse: An LLM-empowered Diagnostic Reasoning System through Harnessing\n",
      "  Outco...\n",
      "50. Diagnostic Systems in the Muon $g-2$ experiment at Fermilab...\n",
      "51. Application of digital regulated Power Supplies for Magnet Control at\n",
      "  the Swis...\n",
      "52. Feasibility of Diagnostics Undulator Studies at ASTA...\n",
      "53. Using complex networks towards information retrieval and diagnostics in\n",
      "  multid...\n",
      "54. Experimental diagnostics of entanglement swapping by a collective\n",
      "  entanglement...\n",
      "55. Two-dimensional time- and space-resolved diagnostic method for\n",
      "  integrated impl...\n",
      "56. Deep convolutional neural networks for multi-scale time-series\n",
      "  classification ...\n",
      "57. Multi-Channel Masked Autoencoder and Comprehensive Evaluations for\n",
      "  Reconstruct...\n",
      "58. Time-Correlated Single-Photon Counting for versatile longitudinal\n",
      "  diagnostics ...\n",
      "59. A Multimodal In Vitro Diagnostic Method for Parkinson's Disease\n",
      "  Combining Faci...\n",
      "60. Fast particle-driven ion cyclotron emission (ICE) in tokamak plasmas and\n",
      "  the c...\n",
      "61. Radiation effects on the Gaia CCDs after 30 months at L2...\n",
      "62. Diagnostic Quality Assessment of Fundus Photographs: Hierarchical Deep\n",
      "  Learnin...\n",
      "63. Am I Infected? Lessons from Operating a Large-Scale IoT Security\n",
      "  Diagnostic Se...\n",
      "64. Guaranteed violation of a Bell inequality without aligned reference\n",
      "  frames or ...\n",
      "65. Nanostructured graphene for spintronics...\n",
      "66. Towards a Magnetically Actuated Laser Scanner for Endoscopic\n",
      "  Microsurgeries...\n",
      "67. A Secure Proxy-based Access Control Scheme for Implantable Medical\n",
      "  Devices...\n",
      "68. The Raspberry Pi Auto-aligner: Machine Learning for Automated Alignment\n",
      "  of Las...\n",
      "69. Training and Profiling a Pediatric Emotion Recognition Classifier on\n",
      "  Mobile De...\n",
      "70. On Medical Device Cybersecurity Compliance in EU...\n",
      "71. Femtosecond electron beam probe of ultrafast electronics...\n",
      "72. Single Langmuir Probe Diagnostics Device...\n",
      "73. Low-Cost Device Prototype for Automatic Medical Diagnosis Using Deep\n",
      "  Learning ...\n",
      "74. Diagnostic reasoning with A-Prolog...\n",
      "75. Measurement and Modeling of Electron Cloud in a Field Free Environment\n",
      "  Using R...\n",
      "76. Spectral analysis for noise diagnostics and filter-based digital error\n",
      "  mitigat...\n",
      "77. Fusion research in a Deuterium-Tritium tokamak...\n",
      "78. Simultaneous 12-Lead Electrocardiogram Synthesis using a Single-Lead ECG\n",
      "  Signa...\n",
      "79. Status of COLDDIAG: A Cold Vacuum Chamber for Diagnostics...\n",
      "80. A portable X-pinch design for x-ray diagnostics of warm dense matter...\n",
      "81. Improving Clinical Diagnosis Performance with Automated X-ray Scan\n",
      "  Quality Enh...\n",
      "82. Photocurrent as a multi-physics diagnostic of quantum materials...\n",
      "83. Zodiac: A Cardiologist-Level LLM Framework for Multi-Agent Diagnostics...\n",
      "84. Detecting Long QT Syndrome and First-Degree Atrioventricular Block using\n",
      "  Singl...\n",
      "85. The environment effect on operation of in-vessel mirrors for plasma\n",
      "  diagnostic...\n",
      "86. Intelligent Automated Diagnosis of Client Device Bottlenecks in Private\n",
      "  Clouds...\n",
      "87. Signal focusing through active transport...\n",
      "88. Low complexity convolutional neural network for vessel segmentation in\n",
      "  portabl...\n",
      "89. Thomson scattering on the Large Plasma Device...\n",
      "90. Elasticity Measurements of Expanded Foams using a Collaborative Robotic\n",
      "  Arm...\n",
      "91. Efficient Detection of Strong-To-Weak Spontaneous Symmetry Breaking via\n",
      "  the Ré...\n",
      "92. BATIS: Bootstrapping, Autonomous Testing, and Initialization System for\n",
      "  Quantu...\n",
      "93. A Deep Neural Network Deployment Based on Resistive Memory Accelerator\n",
      "  Simulat...\n",
      "94. Broadband Angular-selective Mid-infrared Photodetector...\n",
      "95. Neutron diagnostics for the physics of a high-field, compact, $Q\\geq1$\n",
      "  tokamak...\n",
      "96. Time-domain global similarity method for automatic data cleaning for\n",
      "  multi-cha...\n",
      "97. Applications of Deep Learning to Nuclear Fusion Research...\n",
      "98. Efficient and powerful equivalency test on combined mean and variance\n",
      "  with app...\n",
      "99. Simulated assessment of light transport through ischaemic skin flaps...\n",
      "100. Multiscale fabrication of scalable biomimetic 3-D, integrated\n",
      "  micro-nanochanne...\n"
     ]
    }
   ],
   "source": [
    "ns = {\"ns\": \"http://www.w3.org/2005/Atom\"}\n",
    "entries = parse_xml.findall('ns:entry', ns)\n",
    "\n",
    "articles_data = []\n",
    "for entry in entries:\n",
    "    link = entry.find('ns:link[@type=\"application/pdf\"]', ns)\n",
    "    if link is not None and \"href\" in link.attrib:\n",
    "        pdf_url = link.attrib['href']\n",
    "\n",
    "        title = entry.find('ns:title', ns)\n",
    "        title_text = title.text.strip() if title is not None else \"Unknown Title\"\n",
    "\n",
    "        authors = entry.findall('ns:author/ns:name', ns)\n",
    "        author_names = [author.text for author in authors] if authors else [\"Unknown Author\"]\n",
    "\n",
    "        published = entry.find('ns:published', ns)\n",
    "        published_date = published.text[:10] if published is not None else \"Unknown Date\"\n",
    "\n",
    "        summary = entry.find('ns:summary', ns)\n",
    "        summary_text = summary.text.strip() if summary is not None else \"No summary available\"\n",
    "\n",
    "        metadata = {\n",
    "            'title': title_text,\n",
    "            'authors': author_names,\n",
    "            'published': published_date,\n",
    "            'summary': summary_text\n",
    "        }\n",
    "\n",
    "        articles_data.append({\n",
    "            'pdf_url': pdf_url,\n",
    "            'metadata': metadata\n",
    "        })\n",
    "\n",
    "print(f\"Found {len(articles_data)} articles with PDF links\")\n",
    "for i, article in enumerate(articles_data):\n",
    "    print(f\"{i+1}. {article['metadata']['title'][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b26c8e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "dimension = 768\n",
    "model = SentenceTransformer('pritamdeka/S-BioBert-snli-multinli-stsb')\n",
    "chunk_index = faiss.IndexFlatL2(dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0183ac9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def cos_sim(e1, e2):\n",
    "    return np.dot(e1, e2) / (np.linalg.norm(e1) * np.linalg.norm(e2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c86db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanmayshubhgarg/Documents/Projects/DePaulProject/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PyPDF2 import PdfReader\n",
    "import io\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for i, article in enumerate(articles_data):\n",
    "    try:\n",
    "        pdf_response = requests.get(article['pdf_url'], timeout=30)\n",
    "        pdf_response.raise_for_status()\n",
    "\n",
    "        pdf_file = io.BytesIO(pdf_response.content)\n",
    "        pdf_reader = PdfReader(pdf_file)\n",
    "        pdf_text = \"\"\n",
    "\n",
    "        for page in pdf_reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text and page_text.strip():\n",
    "                pdf_text += page_text + \" \"\n",
    "\n",
    "        pdf_text = re.sub(r' {2,}', ' ', pdf_text)\n",
    "        pdf_text = re.sub(r'\\n{3,}', '\\n\\n', pdf_text)\n",
    "        pdf_text = re.sub(r'[\\f\\v\\r]', ' ', pdf_text)\n",
    "        pdf_text = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]', '', pdf_text)\n",
    "        pdf_text = re.sub(r'([.!?])\\s*([A-Z])', r'\\1 \\2', pdf_text)   \n",
    "        pdf_text = pdf_text.strip()\n",
    "\n",
    "        sentences = nltk.sent_tokenize(pdf_text)\n",
    "\n",
    "        article_chunks = []\n",
    "        current_chunk = []\n",
    "        current_chunk_text = \"\"\n",
    "        prev_embedding = None\n",
    "        max_chunk_size = 1500\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence_embedding = model.encode(sentence, convert_to_tensor=True)\n",
    "            sentence_embedding = sentence_embedding.cpu().numpy().astype('float32')\n",
    "\n",
    "            new_chunk = False\n",
    "\n",
    "            if prev_embedding is not None:\n",
    "                similarity = cos_sim(sentence_embedding, prev_embedding)\n",
    "                if similarity < 0.8:\n",
    "                    new_chunk = True\n",
    "            \n",
    "            if len(current_chunk_text.split()) + len(sentence.split()) > max_chunk_size:\n",
    "                new_chunk = True\n",
    "            \n",
    "            if new_chunk:\n",
    "                chunk_text = \" \".join(current_chunk)\n",
    "                chunk_embedding = model.encode(chunk_text, convert_to_tensor=True)\n",
    "                chunk_embedding = chunk_embedding.cpu().numpy().astype('float32')\n",
    "\n",
    "                chunk_data = {\n",
    "                    'text': chunk_text,\n",
    "                    'embedding': chunk_embedding,\n",
    "                    'metadata': article['metadata'],\n",
    "                    'count': len(current_chunk)\n",
    "                }\n",
    "                chunks.append(chunk_data)\n",
    "                current_chunk = []\n",
    "                current_chunk_text = \"\"\n",
    "\n",
    "            current_chunk.append(sentence)\n",
    "            current_chunk_text += sentence + \" \"\n",
    "            prev_embedding = sentence_embedding\n",
    "\n",
    "        if current_chunk:\n",
    "            chunk_text = \" \".join(current_chunk)\n",
    "            chunk_embedding = model.encode(chunk_text, convert_to_tensor=True)\n",
    "            chunk_embedding = chunk_embedding.cpu().numpy().astype('float32')\n",
    "            \n",
    "            chunk_data = {\n",
    "                'text': chunk_text,\n",
    "                'embedding': chunk_embedding,\n",
    "                'article_index': i,\n",
    "                'article_title': article['metadata']['title'],\n",
    "                'sentence_count': len(current_chunk)\n",
    "            }\n",
    "            \n",
    "            article_chunks.append(chunk_data)\n",
    "            chunks.append(chunk_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing article {i+1}: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8660d691",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
